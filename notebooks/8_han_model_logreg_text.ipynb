{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression, Full Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Han-chung\n",
      "[nltk_data]     Lee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../data'))\n",
    "sys.path.append(os.path.abspath('../models'))\n",
    "\n",
    "import pathlib\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = pathlib.Path('../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_json_paths = list(data_root.glob('*.json'))\n",
    "all_json_paths = [str(path) for path in all_json_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['..\\\\data\\\\Kickstarter_2019-01-17T03_20_02_630Z.json',\n",
       " '..\\\\data\\\\Kickstarter_2019-02-14T03_20_04_734Z.json',\n",
       " '..\\\\data\\\\Kickstarter_2019-03-14T03_20_12_200Z.json',\n",
       " '..\\\\data\\\\Kickstarter_2019-04-18T03_20_02_220Z.json',\n",
       " '..\\\\data\\\\Kickstarter_2019-05-16T03_20_20_822Z.json',\n",
       " '..\\\\data\\\\Kickstarter_2019-06-13T03_20_35_801Z.json',\n",
       " '..\\\\data\\\\Kickstarter_2019-07-18T03_20_05_009Z.json',\n",
       " '..\\\\data\\\\Kickstarter_2019-08-15T03_20_03_022Z.json']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_json_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for line in open(all_json_paths[0], 'r', encoding='utf8'):\n",
    "    data.append(json.loads(line))\n",
    "    \n",
    "data = [record['data'] for record in data]\n",
    "raw = pd.DataFrame.from_records(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Naive preprocessing the input data by dropping samples that still have the campaign running,\n",
    "    impute durations and categories, dropping unnecessary features, and one-hot encoding for\n",
    "    training.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "    \"\"\"\n",
    "    # get durations by taking the difference between launch and deadline and transform\n",
    "    # the seconds integer into days.\n",
    "    df['durations'] = round((df.deadline - df.launched_at)/(60*60*24))\n",
    "    \n",
    "    # parse the category feature's json format and extract the first level categories\n",
    "    df['cat_slug'] = df.category.apply(lambda x: x['slug'].split('/')[0])\n",
    "\n",
    "    # map states to 1 for success and 0 for others. Also will drop all 'live' records.\n",
    "    state_dict = {'successful':1, 'failed':0, 'canceled':0, 'suspended':0}\n",
    "    df = df.replace({\"state\": state_dict})\n",
    "    df = df[df.state != 'live']\n",
    "\n",
    "    # drop unused features\n",
    "    df = df[['name', 'blurb', 'goal', 'country', 'durations', 'cat_slug', 'state']]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_names = raw.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw.copy()\n",
    "df = preproc(df)\n",
    "X_col = ['goal', 'durations', 'country', 'cat_slug']\n",
    "X = df[X_col]\n",
    "# need to add .astype('int') to turn it y into int from object. otherise sklearn wont work\n",
    "# https://stackoverflow.com/questions/45346550/valueerror-unknown-label-type-unknown\n",
    "y = df.state.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((172516, 4), (30444, 4), (172516,), (30444,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X.to_numpy(), y.to_numpy(), test_size=0.15, random_state=45)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Transform Text Data Into Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v = Doc2Vec.load(\"../models/d2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_test = word_tokenize(df.name[0].lower())\n",
    "name_v_test = d2v.infer_vector(name_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.7140505 ,  1.157532  ,  2.4612043 ,  2.9914167 , -1.1569723 ,\n",
       "        0.6713491 ,  3.145516  ,  0.14941326, -0.81775874,  1.805982  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_v_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "blurb_test = word_tokenize(df.blurb[0].lower())\n",
    "blurb_v_test = d2v.infer_vector(blurb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.0438399, -3.5648508,  3.5880609,  2.0317347, -3.2620676,\n",
       "       -1.0832196,  6.070351 ,  1.132646 , -1.37094  ,  2.8216355],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blurb_v_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.59 ms ± 257 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "name_v = d2v.infer_vector(word_tokenize(df.name[0].lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15min 1s\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# !!!!!!!!!!!! WARNING! EXPENSIVE CELL, 15 MIN RUNTIMNE !!!!!!!!!!!!!!!!!!\n",
    "#\n",
    "%time asdf = df.name.apply(lambda x: d2v.infer_vector(word_tokenize(x.lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 19min 9s\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# !!!!!!!!!!!! WARNING! EXPENSIVE CELL, 20 MIN RUNTIME !!!!!!!!!!!!!!!!!!!!!\n",
    "#\n",
    "%time blurb_embedding = df.blurb.apply(lambda x: d2v.infer_vector(word_tokenize(x.lower())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = np.vstack(asdf.apply(lambda x: x.flatten()).to_list())\n",
    "blurb = np.vstack(blurb_embedding.apply(lambda x: x.flatten()).to_list())\n",
    "goal = df.goal.to_numpy().reshape(-1, 1)\n",
    "country = df.country.to_numpy().reshape(-1, 1)\n",
    "durations = df.durations.to_numpy().reshape(-1, 1)\n",
    "cat = df.cat_slug.to_numpy().reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((202960, 10),\n",
       " (202960, 10),\n",
       " (202960, 1),\n",
       " (202960, 1),\n",
       " (202960, 1),\n",
       " (202960, 1))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name.shape, blurb.shape, goal.shape, country.shape, durations.shape, cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack([name, blurb, goal, country, durations, cat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202960, 24)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((172516, 24), (30444, 24), (172516,), (30444,))"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y.to_numpy(), test_size=0.15, random_state=45)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X, y):\n",
    "    \"\"\"\n",
    "    Logistic regression model using GridSearchCV. Since GridSearchCV does cross validation internally,\n",
    "    we choose not to split X into training and validation set. We choose to do 5 fold cross validation\n",
    "    during GridSearch. With that, data issplit three ways: 0.68 train, 0.17 validation, and 0.15 test.\n",
    "    We will continue to use OneHotEncoding and StandardScaler in our training pipeline. Since some of\n",
    "    the categorical features have very high cardinality, e.g., funder with 1898 categories, we choose\n",
    "    to take only the top 6 with high cardinality to reduce training time.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : training data\n",
    "    y : target data\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    search.best_estimator : the best Logistic Regression model produced by the GridSearchCV\n",
    "    \"\"\"\n",
    "\n",
    "    logreg = LogisticRegression(solver='lbfgs', multi_class='ovr', random_state=45, max_iter=500)\n",
    "    encoder = OneHotEncoder(categories='auto', handle_unknown='ignore')\n",
    "    scaler = StandardScaler(with_mean=False)\n",
    "\n",
    "    pipe = Pipeline(steps=[('encoder', encoder),\n",
    "                           ('scaler', scaler),\n",
    "                           ('logreg', logreg)\n",
    "                           ])\n",
    "    \n",
    "    param_grid = {\n",
    "        'logreg__C': np.power(10.0, np.arange(3, 10)),\n",
    "    }\n",
    "    \n",
    "    search = GridSearchCV(pipe, param_grid, n_jobs=-1, cv=5)\n",
    "    %time search.fit(X, y)\n",
    "    print(\"Training Score (accuracy): {}\".format(search.best_score_))\n",
    "    print(\"Best Parameters: {}\".format(search.best_params_))\n",
    "    \n",
    "    return search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Full Features\n",
    "We will train using logistic regression with all the features. We first convert the text into document embeddings of size 10. Then we compaign them together with other features into a feature matrix. The matrix will be feed into a scikit-learn pipeline with scaling and encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min 45s\n",
      "Training Score (accuracy): 0.6209395070602147\n",
      "Best Parameters: {'logreg__C': 10000.0}\n"
     ]
    }
   ],
   "source": [
    "model = logistic_regression(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.617987123899619"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Text Features only\n",
    "The full text model has a testing accuracy of only 61.7%, lower than our original 68% logistic regression model without text. Name and blurb might not be have much predictive power to the success of a kickstarter campaign. Lets test it out quickly using name and blurb as input features to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((172516, 20), (30444, 20), (172516,), (30444,))"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_text = np.hstack([name, blurb])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_text, y.to_numpy(), test_size=0.15, random_state=45)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10.4 s\n",
      "Training Score (accuracy): 0.6550580815692457\n",
      "Best Parameters: {'logreg__C': 1000.0}\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(solver='lbfgs', multi_class='ovr', random_state=45, max_iter=500) \n",
    "pipe = Pipeline(steps=[('logreg', logreg)])\n",
    "\n",
    "param_grid = {\n",
    "    'logreg__C': np.power(10.0, np.arange(3, 10)),\n",
    "}   \n",
    "\n",
    "search = GridSearchCV(pipe, param_grid, n_jobs=-1, cv=5)\n",
    "%time search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training Score (accuracy): {}\".format(search.best_score_))\n",
    "print(\"Best Parameters: {}\".format(search.best_params_))\n",
    "\n",
    "model = search.best_estimator_#(solver='lbfgs', multi_class='ovr', random_state=45, max_iter=500).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6558270923663119"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Full Features, Modified Pipeline\n",
    "Interesting. Models with text features only has 65.5% accuracy. So the text features does mean something useful. Perhaps it has something to do with the pipeline, specifically the scalar part. So we will test full features without scalar in the scikit-learn pipeline but scale `goal` and `duration` outside of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((172516, 24), (30444, 24), (172516,), (30444,))"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goal_norm = (goal - goal.mean())/goal.std()\n",
    "dur_norm = (durations - durations.mean())/durations.std()\n",
    "X_text = np.hstack([name, blurb, goal_norm, dur_norm, country, cat])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_text, y.to_numpy(), test_size=0.15, random_state=45)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LogisticRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-2cbdd0bd1e91>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlogreg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'lbfgs'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmulti_class\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ovr'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m45\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mencoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcategories\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'auto'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle_unknown\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m pipe = Pipeline(steps=[('encoder', encoder),\n\u001b[0;32m      5\u001b[0m                        \u001b[1;33m(\u001b[0m\u001b[1;34m'logreg'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogreg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LogisticRegression' is not defined"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(solver='lbfgs', multi_class='ovr', random_state=45, max_iter=500)\n",
    "encoder = OneHotEncoder(categories='auto', handle_unknown='ignore')\n",
    "\n",
    "pipe = Pipeline(steps=[('encoder', encoder),\n",
    "                       ('logreg', logreg)\n",
    "                       ])\n",
    "\n",
    "param_grid = {\n",
    "    'logreg__C': np.power(10.0, np.arange(3, 10)),\n",
    "}\n",
    "\n",
    "search = GridSearchCV(pipe, param_grid, n_jobs=-1, cv=5)\n",
    "%time search.fit(X_train, y_train)\n",
    "print(\"Training Score (accuracy): {}\".format(search.best_score_))\n",
    "print(\"Best Parameters: {}\".format(search.best_params_))\n",
    "\n",
    "model = search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Saving the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filename = '../models/20191022_logreg_68.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Inferencing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import json\n",
    "test1 = '''{\"name\": \"asdfasdfasdf\", \"blurb\": \"asdfasdfasdfadsdfasdfadfasdf\", \"goal\": 800.0, \"country\": \"US\", \"duration\":15.0, \"category\": \"fashion\"}'''\n",
    "test2 = '''{\"goal\": 800.0, \"country\": \"US\", \"duration\":15.0, \"category\": \"fashion\"}'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test1j = json.loads(test2)\n",
    "test1df = pd.DataFrame.from_records(test1j, index=[0])\n",
    "model.predict_proba(test1df.to_numpy())[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 Distributionof testing data\n",
    "Look at the distribution of the prediction for the probability of success."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_pred = model.predict_proba(X_test)\n",
    "plt.hist(y_pred[:,1], bins=40);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(f\"Highest probability w/in test {np.max(y_pred[:,1])}, highest prob sample location is {np.argmax(y_pred[:,1])}\")\n",
    "x_high = X_test[np.argmax(y_pred[:,1])]\n",
    "print(x_high)\n",
    "print(f\"The highest probabilty sample is {x_high}, shape is {x_high.shape})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.predict_proba(x_high.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.predict_proba(X_test[np.argmax(y_pred[:,1])].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "goal, duration, country, category = 2011.0, 67.0, 'US', 'publishing'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test3 = '''{\"goal\": 2011, \"country\": \"US\", \"duration\":67, \"category\": \"publishing\"}'''\n",
    "test3j = json.loads(test3)\n",
    "test3df = pd.DataFrame.from_records(test3j, index=[0], columns=['goal', 'country', 'duration', 'category'])\n",
    "model.predict_proba(test3df.to_numpy())[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
