{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression, Full Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Han-chung\n",
      "[nltk_data]     Lee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../data'))\n",
    "sys.path.append(os.path.abspath('../models'))\n",
    "\n",
    "import pathlib\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = pathlib.Path('../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_json_paths = list(data_root.glob('*.json'))\n",
    "all_json_paths = [str(path) for path in all_json_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['..\\\\data\\\\Kickstarter_2019-01-17T03_20_02_630Z.json',\n",
       " '..\\\\data\\\\Kickstarter_2019-02-14T03_20_04_734Z.json',\n",
       " '..\\\\data\\\\Kickstarter_2019-03-14T03_20_12_200Z.json',\n",
       " '..\\\\data\\\\Kickstarter_2019-04-18T03_20_02_220Z.json',\n",
       " '..\\\\data\\\\Kickstarter_2019-05-16T03_20_20_822Z.json',\n",
       " '..\\\\data\\\\Kickstarter_2019-06-13T03_20_35_801Z.json',\n",
       " '..\\\\data\\\\Kickstarter_2019-07-18T03_20_05_009Z.json',\n",
       " '..\\\\data\\\\Kickstarter_2019-08-15T03_20_03_022Z.json']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_json_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for line in open(all_json_paths[0], 'r', encoding='utf8'):\n",
    "    data.append(json.loads(line))\n",
    "    \n",
    "data = [record['data'] for record in data]\n",
    "raw = pd.DataFrame.from_records(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Naive preprocessing the input data by dropping samples that still have the campaign running,\n",
    "    impute durations and categories, dropping unnecessary features, and one-hot encoding for\n",
    "    training.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "    \"\"\"\n",
    "    # get durations by taking the difference between launch and deadline and transform\n",
    "    # the seconds integer into days.\n",
    "    df['durations'] = round((df.deadline - df.launched_at)/(60*60*24))\n",
    "    \n",
    "    # parse the category feature's json format and extract the first level categories\n",
    "    df['category'] = df.category.apply(lambda x: x['slug'].split('/')[0])\n",
    "\n",
    "    # map states to 1 for success and 0 for others. Also will drop all 'live' records.\n",
    "    state_dict = {'successful':1, 'failed':0, 'canceled':0, 'suspended':0}\n",
    "    df = df.replace({\"state\": state_dict})\n",
    "    df = df[df.state != 'live']\n",
    "\n",
    "    # drop unused features\n",
    "    df = df[['name', 'blurb', 'goal', 'country', 'durations', 'category', 'state']]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw.copy()\n",
    "df = preproc(df)\n",
    "X_col = ['goal', 'durations', 'country', 'category']\n",
    "X = df[X_col]\n",
    "# need to add .astype('int') to turn it y into int from object. otherise sklearn wont work\n",
    "# https://stackoverflow.com/questions/45346550/valueerror-unknown-label-type-unknown\n",
    "y = df.state.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((172516, 4), (30444, 4), (172516,), (30444,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X.to_numpy(), y.to_numpy(), test_size=0.15, random_state=45)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Transform Text Data Into Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v = Doc2Vec.load(\"../models/d2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_test = word_tokenize(df.name[0].lower())\n",
    "name_v_test = d2v.infer_vector(name_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.6723077 ,  1.1827933 ,  2.889591  ,  3.2563348 , -1.398114  ,\n",
       "        0.7121932 ,  3.0553718 , -0.16616717, -0.30374274,  1.5237863 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_v_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "blurb_test = word_tokenize(df.blurb[0].lower())\n",
    "blurb_v_test = d2v.infer_vector(blurb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.2961464, -3.8206117,  3.3928459,  1.992708 , -3.2097907,\n",
       "       -1.1067653,  5.9106565,  1.3232312, -1.1453363,  2.450148 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blurb_v_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.05 ms ± 76.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "name_v = d2v.infer_vector(word_tokenize(df.name[0].lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# !!!!!!!!!!!! WARNING! EXPENSIVE CELL, 15 MIN RUNTIMNE !!!!!!!!!!!!!!!!!!\n",
    "#\n",
    "%time name_embedding = df.name.apply(lambda x: d2v.infer_vector(word_tokenize(x.lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# !!!!!!!!!!!! WARNING! EXPENSIVE CELL, 20 MIN RUNTIME !!!!!!!!!!!!!!!!!!!!!\n",
    "#\n",
    "%time blurb_embedding = df.blurb.apply(lambda x: d2v.infer_vector(word_tokenize(x.lower())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = np.vstack(name_embedding.apply(lambda x: x.flatten()).to_list())\n",
    "blurb = np.vstack(blurb_embedding.apply(lambda x: x.flatten()).to_list())\n",
    "goal = df.goal.to_numpy().reshape(-1, 1)\n",
    "country = df.country.to_numpy().reshape(-1, 1)\n",
    "durations = df.durations.to_numpy().reshape(-1, 1)\n",
    "cat = df.category.to_numpy().reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name.shape, blurb.shape, goal.shape, country.shape, durations.shape, cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack([name, blurb, goal, country, durations, cat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y.to_numpy(), test_size=0.15, random_state=45)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X, y):\n",
    "    \"\"\"\n",
    "    Logistic regression model using GridSearchCV. Since GridSearchCV does cross validation internally,\n",
    "    we choose not to split X into training and validation set. We choose to do 5 fold cross validation\n",
    "    during GridSearch. With that, data issplit three ways: 0.68 train, 0.17 validation, and 0.15 test.\n",
    "    We will continue to use OneHotEncoding and StandardScaler in our training pipeline. Since some of\n",
    "    the categorical features have very high cardinality, e.g., funder with 1898 categories, we choose\n",
    "    to take only the top 6 with high cardinality to reduce training time.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : training data\n",
    "    y : target data\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    search.best_estimator : the best Logistic Regression model produced by the GridSearchCV\n",
    "    \"\"\"\n",
    "\n",
    "    logreg = LogisticRegression(solver='lbfgs', multi_class='ovr', random_state=45, max_iter=500)\n",
    "    encoder = OneHotEncoder(categories='auto', handle_unknown='ignore')\n",
    "    scaler = StandardScaler(with_mean=False)\n",
    "\n",
    "    pipe = Pipeline(steps=[('encoder', encoder),\n",
    "                           ('scaler', scaler),\n",
    "                           ('logreg', logreg)\n",
    "                           ])\n",
    "    \n",
    "    param_grid = {\n",
    "        'logreg__C': np.power(10.0, np.arange(3, 10)),\n",
    "    }\n",
    "    \n",
    "    search = GridSearchCV(pipe, param_grid, n_jobs=-1, cv=5)\n",
    "    %time search.fit(X, y)\n",
    "    print(\"Training Score (accuracy): {}\".format(search.best_score_))\n",
    "    print(\"Best Parameters: {}\".format(search.best_params_))\n",
    "    \n",
    "    return search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Full Features\n",
    "We will train using logistic regression with all the features. We first convert the text into document embeddings of size 10. Then we compaign them together with other features into a feature matrix. The matrix will be feed into a scikit-learn pipeline with scaling and encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = logistic_regression(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Text Features only\n",
    "The full text model has a testing accuracy of only 61.7%, lower than our original 68% logistic regression model without text. Name and blurb might not be have much predictive power to the success of a kickstarter campaign. Lets test it out quickly using name and blurb as input features to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text = np.hstack([name, blurb])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_text, y.to_numpy(), test_size=0.15, random_state=45)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(solver='lbfgs', multi_class='ovr', random_state=45, max_iter=500) \n",
    "pipe = Pipeline(steps=[('logreg', logreg)])\n",
    "\n",
    "param_grid = {\n",
    "    'logreg__C': np.power(10.0, np.arange(3, 10)),\n",
    "}   \n",
    "\n",
    "search = GridSearchCV(pipe, param_grid, n_jobs=-1, cv=5)\n",
    "%time search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training Score (accuracy): {}\".format(search.best_score_))\n",
    "print(\"Best Parameters: {}\".format(search.best_params_))\n",
    "\n",
    "model = search.best_estimator_#(solver='lbfgs', multi_class='ovr', random_state=45, max_iter=500).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Full Features, Modified Pipeline\n",
    "Interesting. Models with text features only has 65.5% accuracy. So the text features does mean something useful. Perhaps it has something to do with the pipeline, specifically the scalar part. So we will test full features without scalar in the scikit-learn pipeline but scale `goal` and `duration` outside of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_norm = (goal - goal.mean())/goal.std()\n",
    "dur_norm = (durations - durations.mean())/durations.std()\n",
    "X_text = np.hstack([name, blurb, goal_norm, dur_norm, country, cat])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_text, y.to_numpy(), test_size=0.15, random_state=45)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logreg = LogisticRegression(solver='lbfgs', multi_class='ovr', random_state=45, max_iter=500)\n",
    "# encoder = OneHotEncoder(categories='auto', handle_unknown='ignore')\n",
    "\n",
    "# pipe = Pipeline(steps=[('encoder', encoder),\n",
    "#                        ('logreg', logreg)\n",
    "#                        ])\n",
    "\n",
    "# param_grid = {\n",
    "#     'logreg__C': np.power(10.0, np.arange(3, 10)),\n",
    "# }\n",
    "\n",
    "# search = GridSearchCV(pipe, param_grid, n_jobs=-1, cv=5)\n",
    "# %time search.fit(X_train, y_train)\n",
    "# print(\"Training Score (accuracy): {}\".format(search.best_score_))\n",
    "# print(\"Best Parameters: {}\".format(search.best_params_))\n",
    "\n",
    "# model = search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.named_steps['logreg'].coef_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.country.value_counts(dropna=False).index.shape, df.category.value_counts(dropna=False).index.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Full Features, Manual Transformation, No Pipeline\n",
    "Looking at the number of coefficients, something is seriously messed up. We are expecting 10 for name, 10 for blurb, 1 for goal, 1 for duration, 22 for country, and 15 for categories. That is a total of 59 parameters. But looking at our coef_, we have 3,448,327. Something went wrong, most probably encoder. We will try to manually encode each of the columns to make it work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(categories='auto', handle_unknown='ignore')\n",
    "cc = ohe.fit_transform(country).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe2 = OneHotEncoder(categories='auto', handle_unknown='ignore')\n",
    "catcat = ohe.fit_transform(cat).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catcat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_norm = (goal - goal.mean())/goal.std()\n",
    "dur_norm = (durations - durations.mean())/durations.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name.shape, blurb.shape, goal_norm.shape, dur_norm.shape, cc.shape, catcat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text = np.hstack([name, blurb, goal_norm, dur_norm, cc, catcat])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_text, y.to_numpy(), test_size=0.15, random_state=45)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(solver='lbfgs', multi_class='ovr', random_state=45, max_iter=500)\n",
    "\n",
    "pipe = Pipeline(steps=[('logreg', logreg)\n",
    "                       ])\n",
    "\n",
    "param_grid = {\n",
    "    'logreg__C': np.power(10.0, np.arange(3, 10)),\n",
    "}\n",
    "\n",
    "search = GridSearchCV(pipe, param_grid, n_jobs=-1, cv=5)\n",
    "%time search.fit(X_train, y_train)\n",
    "print(\"Training Score (accuracy): {}\".format(search.best_score_))\n",
    "print(\"Best Parameters: {}\".format(search.best_params_))\n",
    "\n",
    "model = search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.named_steps['logreg'].coef_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Full Features\n",
    "That's better. Not the number of feature parameters now make sense, the testing accuracy is higher as well, at 0.689. Further, the model size is now only 1.9kB. Now we will try to create a column_transformer for features outside of the doc2vec models. The reason we choose not to incorporate doc2vec into the pipeline yet is due to the speed of the transformation. Embedding the whole name and the whole blurb takes around 25 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['goal', 'durations']\n",
    "numeric_transformer = StandardScaler(with_mean=False)\n",
    "    \n",
    "categorical_features = ['country', 'category']\n",
    "categorical_transformer = OneHotEncoder(categories='auto', handle_unknown='ignore')\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_state = df.iloc[:, :-1]\n",
    "df_no_state.reset_index(inplace=True)\n",
    "df_no_state.drop(\"index\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_state.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_model = preprocessor.fit(df_no_state)\n",
    "ct_filename = '../models/20191023_preproc.sav'\n",
    "pickle.dump(ct_model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_model = pickle.load(open(ct_filename, 'rb'))\n",
    "X_transformed = ct_model.transform(df).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_FF = np.hstack([name, blurb, X_transformed])\n",
    "X_FF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_FF, y.to_numpy(), test_size=0.15, random_state=45)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(solver='lbfgs', multi_class='ovr', random_state=45, max_iter=500)\n",
    "\n",
    "pipe = Pipeline(steps=[('logreg', logreg)])\n",
    "\n",
    "param_grid = {\n",
    "    'logreg__C': np.power(10.0, np.arange(3, 10)),\n",
    "}\n",
    "\n",
    "search = GridSearchCV(pipe, param_grid, n_jobs=-1, cv=5)\n",
    "%time search.fit(X_train, y_train)\n",
    "print(\"Training Score (accuracy): {}\".format(search.best_score_))\n",
    "print(\"Best Parameters: {}\".format(search.best_params_))\n",
    "\n",
    "model = search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.named_steps['logreg'].coef_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../models/20191023_logreg_text_69.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = '''{\"name\": \"This is a test Kickstarter header\", \"blurb\": \"This is an example description of a kickstarter project to test for the API. I would like to thank my wife, parents, and all my loving family members for this to work. I would also like to thank all the Kickstarter team members and project leads for making this possible.\", \n",
    "            \"goal\": 2011.0, \n",
    "            \"country\": \"US\", \n",
    "            \"duration\":67.0, \n",
    "            \"category\": \"publishing\"}'''\n",
    "test1j = json.loads(test1)\n",
    "test1df = pd.DataFrame.from_records(test1j, index=[0], columns=['name', 'blurb', 'goal', 'country', 'duration', 'category'])\n",
    "#model.predict_proba(test3df.to_numpy())[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D2V_FILENAME = \"../models/d2v.model\"\n",
    "CT_FILENAME = '../models/20191023_preproc.sav'\n",
    "MODEL_FILENAME = \"../models/20191023_logreg_text_69.sav\"\n",
    "\n",
    "d2v = Doc2Vec.load(D2V_FILENAME)\n",
    "ct = pickle.load(open(CT_FILENAME, 'rb'))\n",
    "model = pickle.load(open(MODEL_FILENAME, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:,:-1].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "X_none_text = ct_model.transform(test1df).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 Distributionof testing data\n",
    "Look at the distribution of the prediction for the probability of success."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_pred = model.predict_proba(X_test)\n",
    "plt.hist(y_pred[:,1], bins=40);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(f\"Highest probability w/in test {np.max(y_pred[:,1])}, highest prob sample location is {np.argmax(y_pred[:,1])}\")\n",
    "x_high = X_test[np.argmax(y_pred[:,1])]\n",
    "print(x_high)\n",
    "print(f\"The highest probabilty sample is {x_high}, shape is {x_high.shape})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.predict_proba(x_high.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.predict_proba(X_test[np.argmax(y_pred[:,1])].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "goal, duration, country, category = 2011.0, 67.0, 'US', 'publishing'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
